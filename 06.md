# Virtual Machines

What if Kernel itself has errors?

- Monolithic kernel (Linux)
  * no enforced modularity
  * bugs in kernel may be very dangerous
    => arbitrary failures
    => masked problems
- Microkernel
  * enforced modularity
  * but dependencies between components makes it hard to separate things

Trade-off: redesign MK vs new features

## Multiple OS on 1 computer

- Compatibility is a challenge, that's why the abstraction is difficult.
- So we go with Virtualization


Virtual machines (VM)

    [ls][vi][ps][ls]
    [kernel][kernel] - Guest
    [  VM monitor  ] - Host

VM state:

- memory
- PTP register (page table pointer)
- U/K bit

So VM monitor acts like a kernel and every guest kernel is similar to a thread...

Now we have two mappings:

Guest virtual mem. <=> Guest physical mem. <=> Host physical mem.
                   PTP                     PTP

There is a technique to combine those two mappings into one: Shadow page tables

In order to restrict the guest OS from modifying the real memory page containing
their PTP, host OS would mark that real page as read-only.

Virtualizing U/K bit:
- both guest kernel and guest program are now running in the real user mode
- U/K bit effects:
  * privileged instrc.
- trap instrc. and emulate them to the guest


Non-virtualizable instr:
- on x86 not every instruction was trapable
- ex: read cur U/K bit
- para-virtualization - tell guest OS not to execute those
  instructions directly (Xen did it)
  * not able to do it with Windows, no source code
- VMware did binary translation: replaces instruction in binary
- Hardware support



Assignment: hands on valgrind
===

After reading the Eraser paper we have an idea of how valgrind's helgrind analyzes the program.

Q.1: there is clearly a race condition in `put` on line 61 and line 64. Looking for an unoccupied space in the hash-table's bucket we check for an availability of a cell and try to write to it right after successful check. In the time between successful check and the write another thread can try to take the same cell and in the result one of the values will be lost.

Q.2: the easiest fix would be to lock the entire table for the `put`. It would result into correctness but would mean there is no way an additional thread will speed anything up since the lock is table-wise.

Q.3: No, it is not faster.

Q.4: Only one thread can do anything with the table at a given point of time. And the lock actually causes more overhead.

Q.5: Because the `get` phase becomes parallel.

Q.6: Parallel reads in the `get` phase are fine and correct, since all the writes actually ended in the `put` phase.

Q.7: We can lock only bucket wise:

```diff
19a20
> pthread_mutex_t bucket_lock[NBUCKET];
27,29d27
< // The lock that serializes get()'s.
< pthread_mutex_t lock;
<
57a56
>   assert(pthread_mutex_lock(bucket_lock + b) == 0);
64a64
>       assert(pthread_mutex_unlock(bucket_lock + b) == 0);
75d74
<   assert(pthread_mutex_lock(&lock) == 0);
86d84
<   assert(pthread_mutex_unlock(&lock) == 0);
132c130,131
<   assert(pthread_mutex_init(&lock, NULL) == 0);
---
>   for (i = 0; i < NBUCKET; i++)
>     assert(pthread_mutex_init(bucket_lock + i, NULL) == 0);
```

This diff brings the speed-up that correlates with the number of threads used.


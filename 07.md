Performance
===

Requirements:
---

RPC server performance requirements:

Throughput: how many requests can we handle per second?
Latency: what is the delay for processing each request?

They are not inverse. Concurrency can increase throughput but not latency.
Queueing is another thing you can do.

Once your server reaches the full load (under increasing number of concurrent
requests) your throughput will stop growing. Latency on the other hand will
start growing as we queue the requests we can not handle at the moment under
load.


Find bottleneck:
---

Hints:

- measure the utilization of resources (disk, network, cpu, memory)
- modeling the request (how much time each part of request handling takes) ->
  predict performance -> reconsile model with real meassurements
- wild guess


Relax bottleneck:
---

- Fix/optimize application (not 6.033)
- Batching (batch processing)
- Caching (add another cache layer :( )
- Concurrency
  * I/O concurrency
  * Parallel hardware
- Scheduling

OS do all those tricks on the OS level.


Hard Disk performance:
---

- Since rotating the disk is very expensive (seek time takes a lot), then
  batching the reads can give a big speed up on sequential reads
- Caching popular sectors in RAM gives speed up in case of a popular sequence of
  blocks. We can cache the last accessed or most accessed in the last time
  frame. Depends a lot on applications. Linux has an API for apps to give hints.
- Use threads to allow disk access in parallel with CPU execution.
  (I/O concurrency)
- Disk scheduling: Elevator algorithm. Access blocks queued up for the next
  disk revolution in the sorted order. It is not a fair approach but allows to
  increase throughput. (Scheduling and batching).



Performance
===

Requirements:
---

RPC server performance requirements:

Throughput: how many requests can we handle per second?
Latency: what is the delay for processing each request?

They are not inverse. Concurrency can increase throughput but not latency.
Queueing is another thing you can do.

Once your server reaches the full load (under increasing number of concurrent
requests) your throughput will stop growing. Latency on the other hand will
start growing as we queue the requests we can not handle at the moment under
load.


Find bottleneck:
---

Hints:

- measure the utilization of resources (disk, network, cpu, memory)
- modeling the request (how much time each part of request handling takes) ->
  predict performance -> reconsile model with real meassurements
- wild guess


Relax bottleneck:
---

- Fix/optimize application (not 6.033)
- Batching (batch processing)
- Caching (add another cache layer :( )
- Concurrency
  * I/O concurrency
  * Parallel hardware
- Scheduling

OS do all those tricks on the OS level.


Hard Disk performance:
---

- Since rotating the disk is very expensive (seek time takes a lot), then
  batching the reads can give a big speed up on sequential reads
- Caching popular sectors in RAM gives speed up in case of a popular sequence of
  blocks. We can cache the last accessed or most accessed in the last time
  frame. Depends a lot on applications. Linux has an API for apps to give hints.
- Use threads to allow disk access in parallel with CPU execution.
  (I/O concurrency)
- Disk scheduling: Elevator algorithm. Access blocks queued up for the next
  disk revolution in the sorted order. It is not a fair approach but allows to
  increase throughput. (Scheduling and batching).

Hands on MapReduce assignment
===

http://web.mit.edu/6.033/www/assignments/handson-mapreduce.html

Q1: maptask and reducetask arguments control the logic executed in the
particular MapReduce job.

Q2: the `run` method spawns a processes pool specifying the degree of
parallerism, applies given Map method over each section (the split was done on
MapReduce instance init) and storing output in even more splitted files.
Say after the initial split we got 4 chunks of initial data. After we applied
map, we got 4 chunks of mapped data. Now sort the tuples in each of the chunks
and split each chunk to 2 chunks by the sort key (not 50-50 but by key range).
Then it applies given Reduce method over the chunks of first group (with keys in
the first range) and then over chunks of the second group. Since we separated
those by sort key ranges, now we have complete answer in those 2 parts of output
and we just need to merge them (in Merge).

Q3: in `map` method `keyvalue` represents the key of input key-value pair,
`value` represents the whole value input value.
In WordCount `keyvalue` represents the offset of the input and `value` is a
chunk of text.

Q4: in `reduce` method `key` represents one of the unique keys aggregated in
that chunk of reduce job and `keyvalues` represents the set of key-value pairs
associated with given `key`.
In WordCount `key` is a unique word found in text and `keyvalues` is a list
of tuples (word, count) for that key.

Q5: `doMap` will be executed `maptask` times and `reduce` will be executed
`reducetask` times. Because initially we have splitted our input into `maptask`
chunks. And later we splitted each of those into `reducetask` chunks by sort
key.

Q6: each `doMap` will run in parallel. So does each `doReduce`.

Q7: (Total number of bytes) / `maptask`, i.e. 4834757/4 = 1208689.25

Q8: each `doReduce` process will process a different number of key-value pairs
depending on the input but will process (Total number of unique words) / 2
unique keys.

Q9: I don't see any speed up by increasing any of the inputs at all.

Q10:

```python
# Builds a sorted reverse index (word, byte-offsets)
class ReverseIndex(MapReduce):

    def __init__(self, maptask, reducetask, path):
        MapReduce.__init__(self,  maptask, reducetask, path)

    # Produce a (key, value) pair for each title word in value
    def Map(self, keyvalue, value):
        results = []
        i = 0
        n = len(value)
        offset = int(keyvalue)
        while i < n:
            while i < n and value[i] not in string.ascii_letters:
                i += 1
            start = i
            while i < n and value[i] in string.ascii_letters:
                i += 1
            w = value[start:i]
            results.append ((w.lower(), offset + start))
        return results

    # Reduce [(key,value), ...])
    def Reduce(self, key, keyvalues):
        return (key, sorted(pair[1] for pair in keyvalues))
```




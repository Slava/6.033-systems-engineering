P2P systems
===

- no central server
- overlay network
- lookup problem

Downsides of Client/Server architecture:

- Single point of failure (server)
- High management overhead
- No obvious central server
- Lack ability to aggregate clients' resources

P2P design goals
---

- No central servers/every node is a server
- How to discover other nodes and data?
- How to partition data across machines?
- How to make it fault tolerant?


Example: Bittorrent
Goal: distribute big files

Node types in network: trackers, peers, seeds

### Which pieces peers need to download?

- strict order (first to last)?
- rarest first? (to increase the likelihood to distribute it)
- random?
- parallel?

Bittorrent downloads pieces using all this techniques except for the strict
order.

The system is pretty decentralized except for the Tracker.
The way to distribute Tracker is to have a DHT (Distributed Hash Table)
between peers.

There will be some routing algorithm that distribute partial information
about torrents and parts.


Chord protocol
---

If N is the total number of servers in the network, Chrod is

- Efficient: O(log(N)) messages per lookup
- Scalable: O(log(N)) state per node
- Robust: survives massive failure


chord id:

- key identifier = sha1
- node identifier = sha1
- both are uniformly distributed
- both exist in the same id space

Now we have consistent hashing in this space: first node that has higher id than
the key is responsible for storing that key.

The simple lookup algorithm then is for every node to keep a link to its
successor. And when a node needs to find another node responsible for
keeping the key K, we would walk down the ring until we find the first node
higher in rating than the key. This takes O(N) hopes.

Similar to the idea of "binary jumps" (двоичные подъемы), let's store the
information about a node located 1/2 of the ring away from us, 1/4 of the ring
away, 1/8 and so on. Such a table is called "Finget Table".

When a new node arrives, it identifies the closest successor and copies over
the information the new node will now will be responsible for. Nodes
checking with the mentioned successor will be redirected to the new node
later if it makes sense.

In case nodes go down, we need to keep `r` closest successors in this case so
we can fall back on a live successor. Bigger `r` -> more lookups/storage but
higher probablity of robustness.

Other concerns:

- concurrent joins of new nodes
- taking locality and latency into consideration
- heterogeneous nodes
- dishonest nodes


